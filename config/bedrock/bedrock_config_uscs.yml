# Prompt version (version tag to be used from the classification_prompts.yml in MLFlow)
prompts_file: bedrock/uscs_classification_prompts.yml
prompt_version: baseline

# Classification parameter version (version tag to be used from the classification_patterns_bedrock.yml in MLFlow)
pattern_file: bedrock/uscs_classification_patterns_bedrock.yml
pattern_version: baseline

# temperature - answer creativity for the LLM model
temperature: 0.0

# Maximum response tokens
max_tokens: 2048

# Parameter indicating if Reasoning (Chain of Thought) mode should be used. Warning this significantly increases latency
reasoning_mode: False